{"cells": [{"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(range(100))", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200331171559-0000\nKERNEL_ID = d69646da-6621-4b52-a504-3707407d27e4\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.count()", "execution_count": 2, "outputs": [{"output_type": "execute_result", "execution_count": 2, "data": {"text/plain": "100"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.sum()", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "4950"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Functional Programming"}, {"metadata": {}, "cell_type": "code", "source": "def gt50(i):\n    if i>50:\n        return True\n    else: \n        return False", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(4))", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "False\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(344))", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "True\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "gt50 = lambda x:x>50", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(5))", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "False\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from random import shuffle", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "l = list(range(100))", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "shuffle(l)", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(l)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(gt50).collect()", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "[98,\n 84,\n 53,\n 52,\n 62,\n 88,\n 72,\n 83,\n 86,\n 71,\n 93,\n 81,\n 80,\n 59,\n 66,\n 63,\n 60,\n 78,\n 95,\n 77,\n 61,\n 89,\n 85,\n 68,\n 65,\n 76,\n 82,\n 99,\n 69,\n 96,\n 75,\n 74,\n 57,\n 67,\n 54,\n 97,\n 92,\n 56,\n 90,\n 58,\n 55,\n 70,\n 64,\n 87,\n 91,\n 94,\n 79,\n 73,\n 51]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(lambda x:x>50).filter(lambda x:x<75).collect()", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "[53,\n 52,\n 62,\n 72,\n 71,\n 59,\n 66,\n 63,\n 60,\n 61,\n 68,\n 65,\n 69,\n 74,\n 57,\n 67,\n 54,\n 56,\n 58,\n 55,\n 70,\n 64,\n 73,\n 51]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Exercise 3: Working with Dataframes"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import Row", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df = spark.createDataFrame([Row(id=1, value = 'value1'), Row(id=2, value = 'value2')])", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.show()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "+---+------+\n| id| value|\n+---+------+\n|  1|value1|\n|  2|value2|\n+---+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "root\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "#### Registering the dataframe as a query table and issue SQL"}, {"metadata": {}, "cell_type": "code", "source": "df.createOrReplaceTempView('df_view')\n", "execution_count": 24, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_result = spark.sql('select value from df_view where id=2')", "execution_count": 25, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_result.show()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "+------+\n| value|\n+------+\n|value2|\n+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df_result.first().value", "execution_count": 27, "outputs": [{"output_type": "execute_result", "execution_count": 27, "data": {"text/plain": "'value2'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df.count()", "execution_count": 31, "outputs": [{"output_type": "execute_result", "execution_count": 31, "data": {"text/plain": "2"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "len(range(999))", "execution_count": 32, "outputs": [{"output_type": "execute_result", "execution_count": 32, "data": {"text/plain": "999"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "sc.parallelize(range(999)).count()", "execution_count": 33, "outputs": [{"output_type": "execute_result", "execution_count": 33, "data": {"text/plain": "999"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Calculating the mean and median"}, {"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(range(100))", "execution_count": 36, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sortedAndIndex = rdd.sortBy(lambda x: x).zipWithIndex().map(lambda (value,key): (key,value))\nn = sortedAndIndexed.count()\nif (n%2)==1:\n    index = (n-1)/2\n    print sortedAndIndex.lookup(index)\nelse:\n    index1 = (n/2)-1\n    index2 = n/2\n    value1 = sortedAndIndex.lookup(index1)[0]\n    value2 = sortedAndIndex.lookup(index2)[0]\n    print (value1+value2)/2\n    \n        ", "execution_count": 37, "outputs": [{"output_type": "error", "ename": "SyntaxError", "evalue": "invalid syntax (<ipython-input-37-a5d8d302f4ac>, line 1)", "traceback": ["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-a5d8d302f4ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sortedAndIndex = rdd.sortBy(lambda x: x).zipWithIndex().map(lambda (value,key): (key,value))\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}