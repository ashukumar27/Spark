{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "#### Stats Exercise"}, {"metadata": {}, "cell_type": "code", "source": "!rm -f hmp.parquet*\n", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200406113821-0000\nKERNEL_ID = e21a3dd4-26ac-4769-aceb-bc15a252d4fa\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "--2020-04-06 11:38:47--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.114.4\nConnecting to github.com (github.com)|140.82.114.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-04-06 11:38:47--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-04-06 11:38:47--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-04-06 11:38:48 (29.3 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df = spark.read.parquet('hmp.parquet')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.createOrReplaceTempView('df')", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.show()", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.printSchema()", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"select class, count(*) from df group by 1\").show()", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "+--------------+--------+\n|         class|count(1)|\n+--------------+--------+\n| Use_telephone|   15225|\n| Standup_chair|   25417|\n|      Eat_meat|   31236|\n|     Getup_bed|   45801|\n|   Drink_glass|   42792|\n|    Pour_water|   41673|\n|     Comb_hair|   23504|\n|          Walk|   92254|\n|  Climb_stairs|   40258|\n| Sitdown_chair|   25036|\n|   Liedown_bed|   11446|\n|Descend_stairs|   15375|\n|   Brush_teeth|   29829|\n|      Eat_soup|    6683|\n+--------------+--------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.groupby('class').count().show()", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone|15225|\n| Standup_chair|25417|\n|      Eat_meat|31236|\n|     Getup_bed|45801|\n|   Drink_glass|42792|\n|    Pour_water|41673|\n|     Comb_hair|23504|\n|          Walk|92254|\n|  Climb_stairs|40258|\n| Sitdown_chair|25036|\n|   Liedown_bed|11446|\n|Descend_stairs|15375|\n|   Brush_teeth|29829|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Creating a plot"}, {"metadata": {"pixiedust": {"displayParams": {"handlerId": "barChart", "keyFields": "class", "valueFields": "count", "mpld3": "true", "sortby": "Values ASC"}}}, "cell_type": "code", "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 10, "outputs": [{"data": {"text/html": "<style type=\"text/css\">.pd_warning{display:none;}</style><div class=\"pd_warning\"><em>Hey, there's something awesome here! To see it, open this notebook outside GitHub, in a viewer like Jupyter</em></div>", "text/plain": "<IPython.core.display.HTML object>"}, "metadata": {}, "output_type": "display_data"}]}, {"metadata": {}, "cell_type": "code", "source": "### Extracting more info", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark.sql('''\nselect *, max/min as minmaxratio -- compute minmaxration based on already computed values\nfrom (\nselect min(ct) as min, max(ct) as max, mean(ct) as mean, stddev(ct) as stddev from (\nselect count(*) as ct from df group by class))\n''').show()", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "### Using Dataframe API", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import col, min,max,mean, stddev\n\ndf \\\n.groupBy('class') \\\n.count() \\\n.select([\n    min(col(\"count\")).alias('min'),\n    max(col(\"count\")).alias('max'),\n    mean(col(\"count\")).alias('mean'),\n    stddev(col(\"count\")).alias('stddev'),\n    \n]) \\\n.select([\n    col('*'),\n    (col(\"max\")/col(\"min\")).alias('minmaxratio')\n]) \\\n.show()", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql('''\nselect class, count(*) as count from df group by class order by count asc\n''').show()", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n|      Eat_soup| 6683|\n|   Liedown_bed|11446|\n| Use_telephone|15225|\n|Descend_stairs|15375|\n|     Comb_hair|23504|\n| Sitdown_chair|25036|\n| Standup_chair|25417|\n|   Brush_teeth|29829|\n|      Eat_meat|31236|\n|  Climb_stairs|40258|\n|    Pour_water|41673|\n|   Drink_glass|42792|\n|     Getup_bed|45801|\n|          Walk|92254|\n+--------------+-----+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "counts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 17, "outputs": [{"output_type": "error", "ename": "AttributeError", "evalue": "'GroupedData' object has no attribute 'orderBy'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m<ipython-input-17-90266324f1aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'orderBy'"]}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}